# -*- coding: utf-8 -*-
"""MatchPredictions.VNPY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AxI6M3t5prdrhImI5FPA7VkGblpPqIiu
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import pickle as pk
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score
from sklearn.preprocessing import StandardScaler,LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBRegressor
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from keras.models import Sequential
from keras.layers import Dense, Dropout, Input


from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/premier-league-matches.csv'
final_dataset = pd.read_csv(file_path)

final_dataset.head()

# Initialize LabelEncoder
team_encoder = LabelEncoder()

# Fit and transform home and away teams
final_dataset['home_team_encoded'] = team_encoder.fit_transform(final_dataset['Home'])
final_dataset['away_team_encoded'] = team_encoder.transform(final_dataset['Away'])

# Show the encoded values
print(final_dataset[['Home', 'home_team_encoded', 'Away', 'away_team_encoded']].head())

# Initialize columns for total goals scored and conceded
final_dataset['home_team_total_goals_scored'] = 0
final_dataset['away_team_total_goals_scored'] = 0
final_dataset['home_team_total_goals_conceded'] = 0
final_dataset['away_team_total_goals_conceded'] = 0

# Dictionary to keep track of cumulative statistics
team_stats = {}

# Process each game
for index, row in final_dataset.iterrows():
    home_team = row['Home']
    away_team = row['Away']
    home_goals = row['HomeGoals']
    away_goals = row['AwayGoals']
    season = row['Season_End_Year']

    # Ensure dictionaries are initialized for each team
    if home_team not in team_stats:
        team_stats[home_team] = {'season': season, 'total_goals_scored': 0, 'total_goals_conceded': 0}

    if away_team not in team_stats:
        team_stats[away_team] = {'season': season, 'total_goals_scored': 0, 'total_goals_conceded': 0}

    # Update goals scored and conceded
    if team_stats[home_team]['season'] == season:
        team_stats[home_team]['total_goals_scored'] += home_goals
        team_stats[home_team]['total_goals_conceded'] += away_goals
    else:
        # Reset for new season
        team_stats[home_team] = {'season': season, 'total_goals_scored': home_goals, 'total_goals_conceded': away_goals}

    if team_stats[away_team]['season'] == season:
        team_stats[away_team]['total_goals_scored'] += away_goals
        team_stats[away_team]['total_goals_conceded'] += home_goals
    else:
        # Reset for new season
        team_stats[away_team] = {'season': season, 'total_goals_scored': away_goals, 'total_goals_conceded': home_goals}

    # Store cumulative statistics
    final_dataset.at[index, 'home_team_total_goals_scored'] = team_stats[home_team]['total_goals_scored']
    final_dataset.at[index, 'away_team_total_goals_scored'] = team_stats[away_team]['total_goals_scored']
    final_dataset.at[index, 'home_team_total_goals_conceded'] = team_stats[home_team]['total_goals_conceded']
    final_dataset.at[index, 'away_team_total_goals_conceded'] = team_stats[away_team]['total_goals_conceded']

# Show the updated DataFrame
print(final_dataset)

import pandas as pd

# Initialize columns for cumulative statistics
final_dataset['home_team_pts'] = 0
final_dataset['away_team_pts'] = 0
final_dataset['home_team_formPts'] = 0
final_dataset['away_team_formPts'] = 0

# Dictionaries to keep track of cumulative statistics
team_stats = {}
form_points = {'home': {}, 'away': {}}

# Track the last processed season to reset statistics accordingly
last_season = None

# Process each game
for index, row in final_dataset.iterrows():
    season = row['Season_End_Year']
    home_team = row['home_team_encoded']
    away_team = row['away_team_encoded']
    home_goals = row['HomeGoals']
    away_goals = row['AwayGoals']
    home_result = row['FTR'] == 'H'
    away_result = row['FTR'] == 'A'

    # Check if the season has changed
    if season != last_season:
        # Reset statistics for the new season
        team_stats = {}
        form_points = {'home': {}, 'away': {}}
        last_season = season

    # Initialize team stats for new teams
    if home_team not in team_stats:
        team_stats[home_team] = {'points': 0}
    if away_team not in team_stats:
        team_stats[away_team] = {'points': 0}

    if home_team not in form_points['home']:
        form_points['home'][home_team] = []
    if away_team not in form_points['away']:
        form_points['away'][away_team] = []

    # Calculate points
    if home_result:
        team_stats[home_team]['points'] += 3
    elif away_result:
        team_stats[away_team]['points'] += 3
    else:
        team_stats[home_team]['points'] += 1
        team_stats[away_team]['points'] += 1

    # Update cumulative statistics
    final_dataset.at[index, 'home_team_pts'] = team_stats[home_team]['points']
    final_dataset.at[index, 'away_team_pts'] = team_stats[away_team]['points']

    # Update form points
    form_points['home'][home_team].append(3 if home_result else (1 if row['FTR'] == 'D' else 0))
    form_points['away'][away_team].append(3 if away_result else (1 if row['FTR'] == 'D' else 0))

    if len(form_points['home'][home_team]) > 5:
        form_points['home'][home_team].pop(0)
    if len(form_points['away'][away_team]) > 5:
        form_points['away'][away_team].pop(0)

    # Calculate form points (sum of last 5 games)
    final_dataset.at[index, 'home_team_formPts'] = sum(form_points['home'][home_team])
    final_dataset.at[index, 'away_team_formPts'] = sum(form_points['away'][away_team])

# Calculate the difference between home and away team points and form points
final_dataset['diffPts'] = final_dataset['home_team_pts'] - final_dataset['away_team_pts']
final_dataset['diffFormPts'] = final_dataset['home_team_formPts'] - final_dataset['away_team_formPts']

# Show the updated DataFrame
print(final_dataset.tail())

# Encode team names
team_encoder = {}
team_decoder = {}
current_code = 0

# Encode Home and Away Teams
for team in pd.concat([final_dataset['Home'], final_dataset['Away']]).unique():
    if team not in team_encoder:
        team_encoder[team] = current_code
        team_decoder[current_code] = team
        current_code += 1

# Add encoded columns to the DataFrame
final_dataset['home_team_encoded'] = final_dataset['Home'].map(team_encoder)
final_dataset['away_team_encoded'] = final_dataset['Away'].map(team_encoder)

# Print encoded DataFrame
print(final_dataset)

# Print the mappings
print("Team Encoder:", team_encoder)
print("Team Decoder:", team_decoder)

# Optionally save mappings to CSV files for later use
pd.DataFrame(list(team_encoder.items()), columns=['Team', 'Encoded']).to_csv('team_encoder.csv', index=False)
pd.DataFrame(list(team_decoder.items()), columns=['Encoded', 'Team']).to_csv('team_decoder.csv', index=False)

final_dataset['home_team_avg_goals_scored'] = 0
final_dataset['home_team_avg_goals_conceded'] = 0
final_dataset['home_team_win_ratio'] = 0
final_dataset['away_team_avg_goals_scored'] = 0
final_dataset['away_team_avg_goals_conceded'] = 0
final_dataset['away_team_win_ratio'] = 0

# Dictionary to keep track of historical metrics
team_metrics = {}

# Calculate historical metrics
for index, row in final_dataset.iterrows():
    home_team = row['Home']
    away_team = row['Away']
    home_goals = row['HomeGoals']
    away_goals = row['AwayGoals']
    season = row['Season_End_Year']

    # Initialize metrics for new teams
    if home_team not in team_metrics:
        team_metrics[home_team] = {'games': 0, 'goals_scored': 0, 'goals_conceded': 0, 'wins': 0, 'draws': 0}
    if away_team not in team_metrics:
        team_metrics[away_team] = {'games': 0, 'goals_scored': 0, 'goals_conceded': 0, 'wins': 0, 'draws': 0}

    # Update metrics
    team_metrics[home_team]['games'] += 1
    team_metrics[home_team]['goals_scored'] += home_goals
    team_metrics[home_team]['goals_conceded'] += away_goals
    team_metrics[away_team]['games'] += 1
    team_metrics[away_team]['goals_scored'] += away_goals
    team_metrics[away_team]['goals_conceded'] += home_goals

    # Determine wins
    if home_goals > away_goals:
        team_metrics[home_team]['wins'] += 1
    elif home_goals < away_goals:
        team_metrics[away_team]['wins'] += 1
    else:
        team_metrics[home_team]['draws'] += 1
        team_metrics[away_team]['draws'] += 1

    # Calculate averages and win ratios
    home_team_avg_goals_scored = team_metrics[home_team]['goals_scored'] / team_metrics[home_team]['games']
    home_team_avg_goals_conceded = team_metrics[home_team]['goals_conceded'] / team_metrics[home_team]['games']
    home_team_win_ratio = team_metrics[home_team]['wins'] / team_metrics[home_team]['games']

    away_team_avg_goals_scored = team_metrics[away_team]['goals_scored'] / team_metrics[away_team]['games']
    away_team_avg_goals_conceded = team_metrics[away_team]['goals_conceded'] / team_metrics[away_team]['games']
    away_team_win_ratio = team_metrics[away_team]['wins'] / team_metrics[away_team]['games']

    # Update DataFrame
    final_dataset.at[index, 'home_team_avg_goals_scored'] = home_team_avg_goals_scored
    final_dataset.at[index, 'home_team_avg_goals_conceded'] = home_team_avg_goals_conceded
    final_dataset.at[index, 'home_team_win_ratio'] = home_team_win_ratio
    final_dataset.at[index, 'away_team_avg_goals_scored'] = away_team_avg_goals_scored
    final_dataset.at[index, 'away_team_avg_goals_conceded'] = away_team_avg_goals_conceded
    final_dataset.at[index, 'away_team_win_ratio'] = away_team_win_ratio

    # Reset metrics after each season
    if index < len(final_dataset) - 1 and final_dataset.at[index + 1, 'Season_End_Year'] != season:
        team_metrics = {}

print(final_dataset)

import pandas as pd

# Ensure 'final_dataset' DataFrame is sorted by date if it isn't already
final_dataset['Date'] = pd.to_datetime(final_dataset['Date'])
final_dataset.sort_values(by='Date', inplace=True)

# Encode team names
team_encoder = {}
team_decoder = {}
current_code = 0

# Corrected code to concatenate 'Home' and 'Away' columns
for team in pd.concat([final_dataset['Home'], final_dataset['Away']]).unique():
    if team not in team_encoder:
        team_encoder[team] = current_code
        team_decoder[current_code] = team
        current_code += 1

final_dataset['home_team_encoded'] = final_dataset['Home'].map(team_encoder)
final_dataset['away_team_encoded'] = final_dataset['Away'].map(team_encoder)

# Initialize columns for cumulative GD and GDform
final_dataset['home_team_GD_cumulative'] = 0
final_dataset['away_team_GD_cumulative'] = 0
final_dataset['home_team_GDform'] = 0
final_dataset['away_team_GDform'] = 0

# Dictionary to keep track of cumulative GD and GDform
team_GD_cumulative = {}
team_GDform = {}

# Variable to keep track of the current season
current_season = None

# Calculate cumulative GD and GDform
for index, row in final_dataset.iterrows():
    home_team = row['home_team_encoded']
    away_team = row['away_team_encoded']
    home_GD = row['HomeGoals'] - row['AwayGoals']
    away_GD = row['AwayGoals'] - row['HomeGoals']
    season = row['Season_End_Year']

    # Reset cumulative values and GDform lists at the start of a new season
    if season != current_season:
        team_GD_cumulative = {}
        team_GDform = {}
        current_season = season

    # Update cumulative GD for home and away teams
    if home_team not in team_GD_cumulative:
        team_GD_cumulative[home_team] = 0
    if away_team not in team_GD_cumulative:
        team_GD_cumulative[away_team] = 0

    team_GD_cumulative[home_team] += home_GD
    team_GD_cumulative[away_team] += away_GD

    # Update cumulative GD columns
    final_dataset.at[index, 'home_team_GD_cumulative'] = team_GD_cumulative[home_team]
    final_dataset.at[index, 'away_team_GD_cumulative'] = team_GD_cumulative[away_team]

    # Initialize GDform lists if they do not exist for the team
    if home_team not in team_GDform:
        team_GDform[home_team] = []
    if away_team not in team_GDform:
        team_GDform[away_team] = []

    # Append current GD to the list of GDform
    team_GDform[home_team].append(home_GD)
    team_GDform[away_team].append(away_GD)

    # Limit GDform list to the last 5 games
    if len(team_GDform[home_team]) > 5:
        team_GDform[home_team].pop(0)
    if len(team_GDform[away_team]) > 5:
        team_GDform[away_team].pop(0)

    # Calculate GDform for the last 5 games
    home_GDform = sum(team_GDform[home_team])
    away_GDform = sum(team_GDform[away_team])

    # Update GDform columns
    final_dataset.at[index, 'home_team_GDform'] = home_GDform
    final_dataset.at[index, 'away_team_GDform'] = away_GDform

# Show the updated DataFrame
print(final_dataset)

# Print unique teams in Home and Away columns
print("Unique Home Teams:", final_dataset['Home'].unique())
print("Unique Away Teams:", final_dataset['Away'].unique())

# Print the total number of unique teams
unique_teams = pd.concat([final_dataset['Home'], final_dataset['Away']]).unique()
print("Total Unique Teams:", len(unique_teams))

# Print encoded teams to verify
print("Encoded Teams Mapping:", team_encoder)
print("Decoded Teams Mapping:", team_decoder)

# Initialize columns for cumulative metrics
final_dataset['home_team_games_played'] = 0
final_dataset['away_team_games_played'] = 0
final_dataset['home_team_loss_ratio'] = 0
final_dataset['away_team_loss_ratio'] = 0
final_dataset['home_team_draw_ratio'] = 0
final_dataset['away_team_draw_ratio'] = 0

# Dictionary to keep track of cumulative metrics
team_metrics = {}

for index, row in final_dataset.iterrows():
    home_team = row['Home']
    away_team = row['Away']
    home_goals = row['HomeGoals']
    away_goals = row['AwayGoals']
    season = row['Season_End_Year']

    # Initialize metrics for new teams
    if home_team not in team_metrics:
        team_metrics[home_team] = {'games': 0, 'losses': 0, 'draws': 0}
    if away_team not in team_metrics:
        team_metrics[away_team] = {'games': 0, 'losses': 0, 'draws': 0}

    # Update metrics
    team_metrics[home_team]['games'] += 1
    team_metrics[away_team]['games'] += 1

    if home_goals > away_goals:
        team_metrics[home_team]['draws'] += 0
        team_metrics[away_team]['draws'] += 0
        team_metrics[home_team]['losses'] += 0
        team_metrics[away_team]['losses'] += 1
    elif home_goals < away_goals:
        team_metrics[home_team]['draws'] += 0
        team_metrics[away_team]['draws'] += 0
        team_metrics[home_team]['losses'] += 1
        team_metrics[away_team]['losses'] += 0
    else:
        team_metrics[home_team]['draws'] += 1
        team_metrics[away_team]['draws'] += 1
        team_metrics[home_team]['losses'] += 0
        team_metrics[away_team]['losses'] += 0

    # Calculate ratios
    home_team_loss_ratio = team_metrics[home_team]['losses'] / team_metrics[home_team]['games']
    away_team_loss_ratio = team_metrics[away_team]['losses'] / team_metrics[away_team]['games']
    home_team_draw_ratio = team_metrics[home_team]['draws'] / team_metrics[home_team]['games']
    away_team_draw_ratio = team_metrics[away_team]['draws'] / team_metrics[away_team]['games']

    # Update DataFrame
    final_dataset.at[index, 'home_team_games_played'] = team_metrics[home_team]['games']
    final_dataset.at[index, 'away_team_games_played'] = team_metrics[away_team]['games']
    final_dataset.at[index, 'home_team_loss_ratio'] = home_team_loss_ratio
    final_dataset.at[index, 'away_team_loss_ratio'] = away_team_loss_ratio
    final_dataset.at[index, 'home_team_draw_ratio'] = home_team_draw_ratio
    final_dataset.at[index, 'away_team_draw_ratio'] = away_team_draw_ratio

    # Reset metrics after each season
    if index < len(final_dataset) - 1 and final_dataset.at[index + 1, 'Season_End_Year'] != season:
        team_metrics = {}

print(final_dataset)

# Drop columns
columns_to_drop = ['Home','Away','Date']
final_dataset = final_dataset.drop(columns=columns_to_drop)

final_dataset.columns

 # Initialize LabelEncoder
 le = LabelEncoder()

# Fit and transform the 'FTR' column
final_dataset['FTR_encoded'] = le.fit_transform(final_dataset['FTR'])

# Print the DataFrame to see the results
print(final_dataset)

# Print the mapping of labels to integers
print("Label mapping:", dict(zip(le.classes_, le.transform(le.classes_))))

from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBRegressor

final_dataset = final_dataset.drop(['FTR'], axis=1)
final_dataset = final_dataset.drop(['HomeGoals'], axis=1)
final_dataset = final_dataset.drop(['AwayGoals'], axis=1)
# Prepare your dataset
y = final_dataset['FTR_encoded'].astype(int)
X = final_dataset.drop(['FTR_encoded'], axis=1)


# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

import numpy as np
rf = RandomForestClassifier(random_state=0)
rf.fit(X_train_scaled, y_train)

# Feature importance
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")
for f in range(X_train.shape[1]):
    print(f"{X.columns[indices[f]]}: {importances[indices[f]]}")

# Create a DataFrame for feature importance
feature_importance_df = pd.DataFrame({
    'Feature': X.columns[indices],
    'Importance': importances[indices]
})

top_features = feature_importance_df.head(15)
top_feature_names = top_features['Feature'].tolist()
print("Top features:", top_feature_names)

# Create a new DataFrame with only the top features
X_train_selected = X_train[top_feature_names]
X_test_selected = X_test[top_feature_names]

# Scale the selected features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_selected)
X_test_scaled = scaler.transform(X_test_selected)

# Define parameter grids for each model

# Logistic Regression parameter grid for hiyperparameter tuning
lr_param_grid = {
    'C': [0.01,0.1, 1, 10,100],
    'penalty': ['l2'],
    'solver': ['liblinear', 'saga']

}



# Perform Grid Search for Logistic Regression

# Logistic Regression
lr_model = LogisticRegression(max_iter=1000)
lr_grid_search = GridSearchCV(lr_model, lr_param_grid, cv=5, scoring='accuracy', verbose=1)
lr_grid_search.fit(X_train_scaled, y_train)
lr_best_model = lr_grid_search.best_estimator_

# Evaluate the Logistic Regression model
lr_pred = lr_best_model.predict(X_test_scaled)
print("Logistic Regression Best Parameters:")
print(lr_grid_search.best_params_)
print("Logistic Regression Confusion Matrix:")
print(confusion_matrix(y_test, lr_pred))
print("\nLogistic Regression Classification Report:")
print(classification_report(y_test, lr_pred))

# Decision Tree parameter grid
dt_param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20, 30,40],
    'min_samples_split': [2, 5, 10]
}

dt_model = DecisionTreeClassifier(random_state=0)
dt_grid_search = GridSearchCV(dt_model, dt_param_grid, cv=5, scoring='accuracy', verbose=1)
dt_grid_search.fit(X_train_scaled, y_train)
dt_best_model = dt_grid_search.best_estimator_

# Evaluate the Decision Tree model
dt_pred = dt_best_model.predict(X_test_scaled)
print("Decision Tree Best Parameters:")
print(dt_grid_search.best_params_)
print("Decision Tree Confusion Matrix:")
print(confusion_matrix(y_test, dt_pred))
print("\nDecision Tree Classification Report:")
print(classification_report(y_test, dt_pred))

#Training using Neural Networks
  # Infer input shape
input_shape = X_train_scaled.shape[1]

# Build the model
model = Sequential()
model.add(Dense(128, input_shape=(input_shape,), activation='relu'))  # First layer, specify input shape
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))  # Second layer
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))  # Third layer
model.add(Dropout(0.5))
model.add(Dense(16, activation='relu'))  # Fourth layer
model.add(Dropout(0.5))
model.add(Dense(3, activation='softmax'))  # Output layer

# Modify the model compilation to use sparse categorical crossentropy
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model with integer-encoded labels
history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test))


# Evaluate the model
loss, accuracy = model.evaluate(X_test_scaled, y_test)
print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')

import pickle

#Since logistic Regression had the highest accuracy, that is now our best model

# Save to Google Drive
model_path = '/content/drive/My Drive/best_lr_model.pkl'
with open(model_path, 'wb') as file:
    pickle.dump(lr_best_model, file)